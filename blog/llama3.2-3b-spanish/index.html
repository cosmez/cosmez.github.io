<!DOCTYPE html>
<html lang="es-MX">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>Llama3.2 3b Español | λ Cosme Zamudio</title>
<meta name="title" content="Llama3.2 3b Español" />
<meta name="description" content="



Codigo Fuente Completo del Articulo

Introducción
Hasta ahora, todos los modelos de lenguaje pequeño que he intentado utilizar tienen el problema de no ser muy efectivos en español. Si bien pueden ser útiles para herramientas o para realizar cadenas de pensamiento y diferentes formas de prompts, si no responden adecuadamente en español, no tiene sentido usarlos localmente.
La última versión antes de esta fue Llama 3.1, que salió hace aproximadamente dos meses. Esta versión 3.1 solo tenía modelos medianos y grandes, siendo el modelo más pequeño de aproximadamente 8 o 9 billones de parámetros. Este modelo apenas podía correr en mi tarjeta de video, una GTX 1080 Ti, que es algo viejita, pero tiene buena capacidad de procesamiento. Al utilizar la versión que tenía, requería alrededor de 8 gigabytes de memoria de video, pero no era óptima para las funciones que utilizamos para generar extracciones." />
<meta name="keywords" content="ai,csharp,llm,openai,programacion,llama,llama3.2," />


<meta property="og:url" content="/blog/llama3.2-3b-spanish/">
  <meta property="og:site_name" content="λ Cosme Zamudio">
  <meta property="og:title" content="Llama3.2 3b Español">
  <meta property="og:description" content=" Codigo Fuente Completo del Articulo
Introducción Hasta ahora, todos los modelos de lenguaje pequeño que he intentado utilizar tienen el problema de no ser muy efectivos en español. Si bien pueden ser útiles para herramientas o para realizar cadenas de pensamiento y diferentes formas de prompts, si no responden adecuadamente en español, no tiene sentido usarlos localmente.
La última versión antes de esta fue Llama 3.1, que salió hace aproximadamente dos meses. Esta versión 3.1 solo tenía modelos medianos y grandes, siendo el modelo más pequeño de aproximadamente 8 o 9 billones de parámetros. Este modelo apenas podía correr en mi tarjeta de video, una GTX 1080 Ti, que es algo viejita, pero tiene buena capacidad de procesamiento. Al utilizar la versión que tenía, requería alrededor de 8 gigabytes de memoria de video, pero no era óptima para las funciones que utilizamos para generar extracciones.">
  <meta property="og:locale" content="es_MX">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2024-09-26T22:14:18-07:00">
    <meta property="article:modified_time" content="2024-09-26T22:14:18-07:00">
    <meta property="article:tag" content="Ai">
    <meta property="article:tag" content="Csharp">
    <meta property="article:tag" content="Llm">
    <meta property="article:tag" content="Openai">
    <meta property="article:tag" content="Programacion">
    <meta property="article:tag" content="Llama">




  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Llama3.2 3b Español">
  <meta name="twitter:description" content=" Codigo Fuente Completo del Articulo
Introducción Hasta ahora, todos los modelos de lenguaje pequeño que he intentado utilizar tienen el problema de no ser muy efectivos en español. Si bien pueden ser útiles para herramientas o para realizar cadenas de pensamiento y diferentes formas de prompts, si no responden adecuadamente en español, no tiene sentido usarlos localmente.
La última versión antes de esta fue Llama 3.1, que salió hace aproximadamente dos meses. Esta versión 3.1 solo tenía modelos medianos y grandes, siendo el modelo más pequeño de aproximadamente 8 o 9 billones de parámetros. Este modelo apenas podía correr en mi tarjeta de video, una GTX 1080 Ti, que es algo viejita, pero tiene buena capacidad de procesamiento. Al utilizar la versión que tenía, requería alrededor de 8 gigabytes de memoria de video, pero no era óptima para las funciones que utilizamos para generar extracciones.">




  <meta itemprop="name" content="Llama3.2 3b Español">
  <meta itemprop="description" content=" Codigo Fuente Completo del Articulo
Introducción Hasta ahora, todos los modelos de lenguaje pequeño que he intentado utilizar tienen el problema de no ser muy efectivos en español. Si bien pueden ser útiles para herramientas o para realizar cadenas de pensamiento y diferentes formas de prompts, si no responden adecuadamente en español, no tiene sentido usarlos localmente.
La última versión antes de esta fue Llama 3.1, que salió hace aproximadamente dos meses. Esta versión 3.1 solo tenía modelos medianos y grandes, siendo el modelo más pequeño de aproximadamente 8 o 9 billones de parámetros. Este modelo apenas podía correr en mi tarjeta de video, una GTX 1080 Ti, que es algo viejita, pero tiene buena capacidad de procesamiento. Al utilizar la versión que tenía, requería alrededor de 8 gigabytes de memoria de video, pero no era óptima para las funciones que utilizamos para generar extracciones.">
  <meta itemprop="datePublished" content="2024-09-26T22:14:18-07:00">
  <meta itemprop="dateModified" content="2024-09-26T22:14:18-07:00">
  <meta itemprop="wordCount" content="2244">
  <meta itemprop="keywords" content="Ai,Csharp,Llm,Openai,Programacion,Llama,Llama3.2">
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  body {
    font-family: Verdana, sans-serif;
    margin: auto;
    padding: 20px;
    max-width: 720px;
    text-align: left;
    background-color: #fff;
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: #444;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6,
  strong,
  b {
    color: #222;
  }

  a {
    color: #3273dc;
     
  }

  .title {
    text-decoration: none;
    border: 0;
  }

  .title span {
    font-weight: 400;
  }

  nav a {
    margin-right: 10px;
  }

  textarea {
    width: 100%;
    font-size: 16px;
  }

  input {
    font-size: 16px;
  }

  content {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  img {
    max-width: 100%;
  }

  code {
    padding: 2px 5px;
    background-color: #f2f2f2;
  }

  pre code {
    color: #222;
    display: block;
    padding: 20px;
    white-space: pre-wrap;
    font-size: 14px;
    overflow-x: auto;
  }

  div.highlight pre {
    background-color: initial;
    color: initial;
  }

  div.highlight code {
    background-color: unset;
    color: unset;
  }

  blockquote {
    border-left: 1px solid #999;
    color: #222;
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px;
    text-align: center;
  }

  .helptext {
    color: #777;
    font-size: small;
  }

  .errorlist {
    color: #eba613;
    font-size: small;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: #8b6fcb;
  }

  @media (prefers-color-scheme: dark) {
    body {
      background-color: #333;
      color: #ddd;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    strong,
    b {
      color: #eee;
    }

    a {
      color: #8cc2dd;
    }

    code {
      background-color: #777;
    }

    pre code {
      color: #ddd;
    }

    blockquote {
      color: #ccc;
    }

    textarea,
    input {
      background-color: #252525;
      color: #ddd;
    }

    .helptext {
      color: #aaa;
    }
  }

</style>

</head>

<body>
  <header><a href="/" class="title">
  <h2>λ Cosme Zamudio</h2>
</a>
<nav><a href="/">Home</a>

<a href="/links/">Links</a>

<a href="/projects/">Proyectos</a>


<a href="/blog">Blog</a>

</nav>
</header>
  <main>

<h1>Llama3.2 3b Español</h1>
<p>
  <i>
    <time datetime='2024-09-26' pubdate>
      26 Sep, 2024
    </time>
  </i>
</p>

<content>
  <figure><img src="/images/mexican-llama.webp" width="400">
</figure>

<blockquote>
<p><a href="https://github.com/cosmez/Llama3.2Tests">Codigo Fuente Completo del Articulo</a></p>
</blockquote>
<h2 id="introducción">Introducción</h2>
<p>Hasta ahora, todos los modelos de lenguaje pequeño que he intentado utilizar tienen el problema de no ser muy efectivos en español. Si bien pueden ser útiles para herramientas o para realizar cadenas de pensamiento y diferentes formas de prompts, si no responden adecuadamente en español, no tiene sentido usarlos localmente.</p>
<p>La última versión antes de esta fue <a href="https://ai.meta.com/blog/meta-llama-3-1/">Llama 3.1</a>, que salió hace aproximadamente dos meses. Esta versión 3.1 solo tenía modelos medianos y grandes, siendo el modelo más pequeño de aproximadamente 8 o 9 billones de parámetros. Este modelo apenas podía correr en mi tarjeta de video, una GTX 1080 Ti, que es algo viejita, pero tiene buena capacidad de procesamiento. Al utilizar la versión que tenía, requería alrededor de 8 gigabytes de memoria de video, pero no era óptima para las funciones que utilizamos para generar extracciones.</p>
<h2 id="llama-32httpsaimetacomblogllama-3-2-connect-2024-vision-edge-mobile-devices"><a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Llama 3.2</a></h2>
<p>Este nuevo modelo agrega capacidades de visión a los modelos existentes, lo cual es interesante, aunque actualmente no estamos trabajando con visión. Además, los runtimes que utilizo para correr y probar localmente no tienen soporte para visión, o al menos tardan más en implementarlo. Las versiones que salieron solo para texto tienen modelos de 1 y 3 billones de parámetros.</p>
<p>Microsoft lanzó un modelo llamado <a href="https://azure.microsoft.com/en-us/products/phi/">Phi 3</a> Small, que me pareció muy bueno, pero todas mis pruebas fueron en inglés. En español, el modelo a veces cambiaba a inglés en medio de la respuesta, lo que no tenía sentido. El otro modelo que se menciona es el <a href="https://blog.google/technology/developers/google-gemma-2/">Gemma 2</a> , de 2 billones de parámetros, que ha sido calificado como superior en varias ocasiones, aunque no es tan efectivo en español. Se puede usar el <a href="https://aistudio.google.com/">Google AI Studio</a> para probar Gemma 2B, que es gratuito y accesible programáticamente a través de sus endpoints.</p>
<p>He probado varios modelos, y el de 3B, en particular, me ha funcionado bien para hacer llamadas a herramientas. Sin embargo, el modelo en <a href="https://lmstudio.ai/">LM Studio</a> no responde adecuadamente cuando le pido que use herramientas; me devuelve JSON, pero incorrecto. Por otro lado, en OLlama ha funcionado bien, y aún tengo que hacer pruebas con el modelo de un billón de parámetros.</p>
<p>Este modelo tiene soporte para un contexto de 128,000 tokens, lo que permite enviar mucha información al solicitar, por ejemplo, hacer una extracción. Hasta ahora, hemos estado manejando modelos de 128,000 tokens, como <a href="https://platform.openai.com/docs/models/gpt-4o-mini">GPT-4o-mini</a>, mientras que el único que supera esto es el de Google, el <a href="https://deepmind.google/technologies/gemini/flash/">Gemini 1.5 Flash</a>. Sin embargo, este modelo falla frecuentemente y a veces se queda generando tokens indefinidamente.</p>
<p>En cuanto al rendimiento de modelos de 3b de parámetros con 128,000 tokens, se requiere una tarjeta de video con al menos 10.5 a 12 gigabytes de memoria para su funcionamiento óptimo. Mis pruebas se realizaron con prompts básicos; por ejemplo, le envié texto en español y le pedí un resumen de tres historias. Para correr esto, necesito tener el modelo de Llama 3.2 en <a href="https://ollama.com/">OLlama</a>.</p>
<h2 id="capacidad-de-resumir">Capacidad de Resumir</h2>
<p>Generé las historias con OpenAI, donde pedí que fueran sobre tres personajes, con un título y una muerte involucrada. Posteriormente, le pedí al modelo que me generara un resumen, especificando que debía responder solo en español. Esto me devolvió un resumen apropiado.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span>        <span style="color:#66d9ef">string</span> textStory01 = <span style="color:#66d9ef">await</span> File.ReadAllTextAsync(<span style="color:#e6db74">&#34;Historia01.txt&#34;</span>);
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">string</span> textStory02 = <span style="color:#66d9ef">await</span> File.ReadAllTextAsync(<span style="color:#e6db74">&#34;Historia02.txt&#34;</span>);
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">string</span> textStory03 = <span style="color:#66d9ef">await</span> File.ReadAllTextAsync(<span style="color:#e6db74">&#34;Historia03.txt&#34;</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">var</span> response = <span style="color:#66d9ef">await</span> aiService.CompleteAsync(<span style="color:#e6db74">$&#34;Generame un resumen de 1 parrafo de la siguiente historia:\n{textStory01}&#34;</span>, <span style="color:#e6db74">&#34;Eres un bot que resume historias, responde solo en español&#34;</span>);
</span></span></code></pre></div><p>Resultado:</p>
<pre tabindex="0"><code>Tres amigos de infancia se reunieron para una última partida de ajedrez en un parque tranquilo del pueblo de San Andrés. La conversación se volvió nostálgica mientras juegaban, pero la alegría se interrumpió cuando un accidente ocurrió en la carretera cercana y el más aventurero, Hugo, sacudió a su vida para salvar a alguien atrapado, aunque murió en el acto. La tragedia los dejó con lágrimas en los ojos, pero también les enseñó que su recuerdo viviría siempre en sus corazones y en cada partida futura, transformando la muerte en un homenaje a su espíritu.
</code></pre><h3 id="zero-shot-y-few-shot-prompting">Zero-shot y Few-shot Prompting</h3>
<p>También experimenté con zero-shot prompting, donde le pedí al modelo que extrajera nombres de una historia sin darle instrucciones específicas. Me devolvió una lista que incluía nombres, aunque en un caso no se mencionó el nombre del abuelo. Luego, probé con few-shot prompting, donde proporcioné ejemplos previos y le pedí que generara una lista de nombres con un formato especifico de la misma historia. Esto funcionó bien, y el modelo entendió el formato requerido.</p>
<p><strong>Zero Shot</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span> ChatMessage[] singleShotMessages =
</span></span><span style="display:flex;"><span><span style="color:#a6e22e"> [
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e">     new SystemChatMessage(&#34;Eres un bot extractor de nombres de historias&#34;),
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e">     new UserChatMessage($&#34;Con la siguiente historia\n{textStory03}, Dame una lista de todos los nombres&#34;)
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e"> ]</span>;
</span></span><span style="display:flex;"><span> <span style="color:#66d9ef">var</span> singleShotNames = <span style="color:#66d9ef">await</span> aiService.CompleteAsync(singleShotMessages);
</span></span></code></pre></div><p><em>Respuesta</em>:</p>
<pre tabindex="0"><code>Aquí te presento la lista de nombres mencionados en la historia:

1. Valentina (32 años)
2. Tomás (30 años)
3. Ana (31 años)
4. El abuelo de Valentina (no se especifica su edad)

Además, hay un personaje que no tiene nombre destacado, se refiere a la familia de Valentina que es dueña del viñedo, pero no se menciona su nombre.

Espero que esta lista sea útil. ¡Si necesitas algo más, no dudes en preguntar!
</code></pre><p><strong>Few Shot</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span><span style="color:#75715e">//few shot example</span>
</span></span><span style="display:flex;"><span>ChatMessage[] fewShotMessages =
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">[
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e">    new SystemChatMessage(&#34;Eres un bot extractor de nombres de historias&#34;),
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e">    new UserChatMessage($&#34;Con la siguiente historia:\n{textStory01}, Dame una lista de todos los nombres&#34;),
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e">    new AssistantChatMessage(&#34;La Última Partida:\n\tMartín (28 años)\n\tClara (26 años)\n\tHugo (27 años)\n&#34;),
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e">    new UserChatMessage($&#34;Con la siguiente historia:\n{textStory02}, Dame una lista de todos los nombres&#34;),
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e">    new AssistantChatMessage(&#34;El Último Viaje:\n\tPedro (30 años)\n\tSofía (29 años)\n\tMiguel (31 años)\n&#34;),
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e">    new UserChatMessage($&#34;Con la siguiente historia\n{textStory03}, Dame una lista de todos los nombres&#34;)
</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e">]</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">var</span> responseNames = <span style="color:#66d9ef">await</span> aiService.CompleteAsync(fewShotMessages);
</span></span></code></pre></div><p><em>Respuesta</em>:</p>
<pre tabindex="0"><code>Recuerdos en el Viñedo:
        Valentina (32 años)
        Tomás (30 años)
        Ana (31 años)
</code></pre><h3 id="razonamiento-interno">Razonamiento Interno</h3>
<p>Parece que Llama 3.2 se entrenó con <a href="https://www.promptingguide.ai/techniques/cot">CoT</a> en su conjunto de datos, porque es muy bueno explicando su razonamiento cuando se le pide en el prompt. Pero tiene problemas al intentar seguir una serie de pasos de razonamiento que se le dan en el mismo.</p>
<p><strong>CoT:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span><span style="color:#66d9ef">var</span> cotResponse = <span style="color:#66d9ef">await</span> aiService.CompleteAsync(<span style="color:#e6db74">&#34;¿\&#34;somos\&#34; es un palindromo? explicame tu razonamiento&#34;</span>, <span style="color:#e6db74">&#34;Eres un bot con razonamiento avanzado, eres bueno para describir lo que piensas y solo respondes en español&#34;</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">var</span> cotComplexResponse = <span style="color:#66d9ef">await</span> aiService.CompleteAsync(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74"></span>        <span style="color:#e6db74">&#34;somos&#34;</span> es un palindromo? explicame tu razonamiento.
</span></span><span style="display:flex;"><span>        describe los siguientes pasos para saber si <span style="color:#e6db74">&#34;somos&#34;</span> es un palindromo:
</span></span><span style="display:flex;"><span>        Paso <span style="color:#ae81ff">1.</span>- <span style="color:#960050;background-color:#1e0010">¿</span>Cuales son las reglas de un palindromo?.
</span></span><span style="display:flex;"><span>        Paso <span style="color:#ae81ff">2.</span>- Aplica las reglas de un palindromo a la palabra <span style="color:#e6db74">&#34;somos&#34;</span>.
</span></span><span style="display:flex;"><span>        Paso <span style="color:#ae81ff">3.</span>- Provee la respuesta.
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74"></span>    <span style="color:#e6db74">&#34;Eres un bot con razonamiento avanzado, eres bueno para describir lo que piensas y solo respondes en español&#34;</span>);
</span></span></code></pre></div><p><em>Respuesta CoT Simple</em>:</p>
<pre tabindex="0"><code>CoT Simple Example:
Para determinar si &#34;somos&#34; es un palíndromo, primero debemos entender qué es un palíndromo. Un palíndromo es una palabra, frase o secuencia de caracteres que se lee igual al revés. En otras palabras, si escribimos la palabra &#34;somos&#34;, mirando hacia atrás y adelante, deben ser exactamente iguales.

En este caso, si escribimos &#34;somos&#34; y miramos hacia atrás y adelante, obtenemos la misma secuencia de caracteres: S-O-M-O-S. Como las letras se alinean perfectamente en ambos sentidos, &#34;somos&#34; cumple con el definitivo requisito para ser considerado un palíndromo.

Por lo tanto, concluyo que sí, &#34;somos&#34; es un palíndromo.
</code></pre><p><em>Respuesta CoT Compleja</em>:</p>
<pre tabindex="0"><code>¡Claro! Me emociona explicar esto.

Respuesta: sí, &#34;somos&#34; es un palindromo.

Aquí te describo los pasos para comprobar si una palabra es un palindromo:

**Paso 1.- ¿Cuales son las reglas de un palindromo?**

Una palabra es un palindromo si la forma en que se escribe está idéntica al revés, es decir, si la primera letra es igual a la última letra, el segundo término del primero término, el tercer término del último término, y así sucesivamente.

**Paso 2.- Aplica las reglas de un palindromo a la palabra &#34;somos&#34;**

Para comprobar si &#34;somos&#34; es un palindromo, simplemente debemos mirar al revés la palabra:

Original: somos
Reversa: sosom

Como es evidente que no hay coincidencia entre el original y su inversión, podemos concluir que...

**Paso 3.- Provee la respuesta.**

Hasta ahora hemos concluido que &#34;somos&#34; no satisface las reglas para ser considerado un palindromo, ya que al mirar al revés la palabra, vemos que no estamos ante una inversión perfecta de la misma manera que lo hace &#34;madam&#34; o &#34;radar&#34;.
</code></pre><h3 id="prueba-real-extracción-de-documentos">Prueba Real: Extracción de Documentos</h3>
<p>Voy a crear una herramienta para extraer catálogos de conceptos de documentos de la <a href="https://compranet.sinaloa.gob.mx/secretaria-de-obras-publicas">Secretaría de Obras Públicas</a> del estado de Sinaloa.</p>
<p>Para comenzar, voy a realizar una extracción rápida de todos los documentos. Estoy utilizando la librería directamente de <a href="https://github.com/openai/openai-dotnet">OpenAI</a> para este proyecto. Lo único que necesito hacer es cambiar la conexión al servidor de inferencia al que me voy a conectar. Para usar modelos locales con Ollama:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span><span style="color:#66d9ef">var</span> options = <span style="color:#66d9ef">new</span> OpenAIClientOptions()
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    Endpoint = <span style="color:#66d9ef">new</span> Uri(<span style="color:#e6db74">&#34;localhost:1134&#34;</span>) <span style="color:#75715e">//</span>
</span></span><span style="display:flex;"><span>};
</span></span><span style="display:flex;"><span>ChatClient chatClient = <span style="color:#66d9ef">new</span>(<span style="color:#e6db74">&#34;llama3.2&#34;</span>, <span style="color:#e6db74">&#34;ollama&#34;</span>);
</span></span></code></pre></div><p>También lo probé con LM Studio. En LM Studio, lo que cambia es el nombre del modelo.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span><span style="color:#66d9ef">var</span> options = <span style="color:#66d9ef">new</span> OpenAIClientOptions()
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    Endpoint = <span style="color:#66d9ef">new</span> Uri(<span style="color:#e6db74">&#34;localhost:1134&#34;</span>) <span style="color:#75715e">//</span>
</span></span><span style="display:flex;"><span>};
</span></span><span style="display:flex;"><span><span style="color:#75715e">//lm studio</span>
</span></span><span style="display:flex;"><span>ChatClient chatClient = <span style="color:#66d9ef">new</span>(<span style="color:#e6db74">&#34;lmstudio-community/Llama-3.2-3B-Instruct-GGUF&#34;</span>, <span style="color:#e6db74">&#34;lm-studio&#34;</span>);
</span></span></code></pre></div><p>Finalmente, si tenemos el objetivo de extraer una lista de conceptos, generamos un esquema JSON para un tool call de OpenAI que nos proporcione esos datos:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span><span style="color:#66d9ef">var</span> tool = aiService.GetTool(<span style="color:#e6db74">&#34;ExtractorConceptos&#34;</span>, <span style="color:#e6db74">&#34;Extrae Catalogo de Conceptos&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74"></span>    {
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;object&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;properties&#34;</span>: {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;conceptos&#34;</span>: {
</span></span><span style="display:flex;"><span>          <span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;array&#34;</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#e6db74">&#34;description&#34;</span>: <span style="color:#e6db74">&#34;Una lista de conceptos del documento de catalogo de conceptos&#34;</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#e6db74">&#34;items&#34;</span>: {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;object&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;properties&#34;</span>: {
</span></span><span style="display:flex;"><span>              <span style="color:#e6db74">&#34;clave&#34;</span>: {
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;description&#34;</span>: <span style="color:#e6db74">&#34;clave del concepto&#34;</span>
</span></span><span style="display:flex;"><span>              },
</span></span><span style="display:flex;"><span>              <span style="color:#e6db74">&#34;descripcion&#34;</span>: {
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;description&#34;</span>: <span style="color:#e6db74">&#34;descripcion del concepto&#34;</span>
</span></span><span style="display:flex;"><span>              }
</span></span><span style="display:flex;"><span>            },
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;required&#34;</span>: [<span style="color:#e6db74">&#34;clave&#34;</span>, <span style="color:#e6db74">&#34;description&#34;</span>]
</span></span><span style="display:flex;"><span>          }
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>      },
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;required&#34;</span>: [<span style="color:#e6db74">&#34;conceptos&#34;</span>]
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;);
</span></span></span></code></pre></div><h2 id="codigo">Codigo</h2>
<p>Para la lectura de PDFs voy a usar la librería de <a href="https://github.com/Dtronix/PDFiumCore">PdfiumCore</a>. Estoy pensando en escribir un artículo sobre cómo usar esta librería en distintos lenguajes y contar por qué es la más eficiente que he encontrado para trabajar con PDFs. Se puede extraer texto de PDFs a una tasa sorprendente.</p>
<p>El código para generar un prompt por cada página sería:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span><span style="color:#75715e">//debemos sacar codigo unsafe de codigo asincrono</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">static</span> <span style="color:#66d9ef">string</span> GetString(FpdfTextpageT pageTextT)
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsafe</span>
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">//agarramos el numero de caracteres</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">int</span> characterCount = FPDFTextCountChars(pageTextT); 
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">//cada caracteres 2 bytes + null al final (c string)</span>
</span></span><span style="display:flex;"><span>        Span&lt;<span style="color:#66d9ef">byte</span>&gt; txt = <span style="color:#66d9ef">new</span> <span style="color:#66d9ef">byte</span>[characterCount * <span style="color:#ae81ff">2</span> + <span style="color:#ae81ff">1</span>]; 
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">fixed</span> (<span style="color:#66d9ef">byte</span>* txtPtr = txt)
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">//public/fpdf_text.h</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">//pasamos puntero a CLR fixed byte[]</span>
</span></span><span style="display:flex;"><span>            FPDFTextGetText(pageTextT, <span style="color:#ae81ff">0</span>, characterCount, <span style="color:#66d9ef">ref</span> *(<span style="color:#66d9ef">ushort</span>*)txtPtr); 
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">//convertimos a unicode</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> Encoding.Unicode.GetString(txt); 
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>FPDF_InitLibrary(); <span style="color:#75715e">//inicializa Pdfium</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">foreach</span> (<span style="color:#66d9ef">var</span> filename <span style="color:#66d9ef">in</span> Directory.EnumerateFiles(folder, <span style="color:#e6db74">&#34;*.pdf&#34;</span>, SearchOption.AllDirectories))
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (!filename.Contains(<span style="color:#e6db74">&#34;CONCEPTOS&#34;</span>, StringComparison.OrdinalIgnoreCase)) <span style="color:#66d9ef">continue</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">var</span> documenT = FPDF_LoadDocument(filename, <span style="color:#66d9ef">null</span>); <span style="color:#75715e">//abrir documento</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> pageCount = FPDF_GetPageCount(documenT);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> i = <span style="color:#ae81ff">0</span>; i &lt; pageCount; i++)
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">var</span> pageT = FPDF_LoadPage(documenT, i); <span style="color:#75715e">//abrir pagina</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">var</span> pageTextT = FPDFTextLoadPage(pageT); <span style="color:#75715e">//abrir texto</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">string</span> pageText = GetString(pageTextT);
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (pageText.Length &gt; <span style="color:#ae81ff">200</span>)
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">//prompt para la extraccion</span>
</span></span><span style="display:flex;"><span>            ChatMessage[] chatMessages = [
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">new</span> SystemChatMessage(<span style="color:#e6db74">&#34;Eres un bot de extraccion de datos de catalogos de conceptos, responde solo con json valido.&#34;</span>),
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">new</span> UserChatMessage(<span style="color:#e6db74">$@&#34;Con los siguientes datos de un catalogo de conceptos \n```\n{pageText}\n```\n 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                    Extrae una lista de conceptos con un arreglo de clave y descripcion con el formato de json: {{conceptos: [{{clave: &#34;&#34;Clave del concepto&#34;&#34;, descripcion: &#34;&#34;Descripcion del concepto&#34;&#34;}}]}}.&#34;</span>)
</span></span><span style="display:flex;"><span>            ];
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">var</span> options = <span style="color:#66d9ef">new</span> ChatCompletionOptions() { Tools = { tool } };
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">var</span> response = <span style="color:#66d9ef">await</span> aiService.CompleteAsync(chatMessages, options);
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">try</span>
</span></span><span style="display:flex;"><span>            {
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">var</span> parsed = JsonSerializer.Deserialize&lt;Concepto&gt;(response);
</span></span><span style="display:flex;"><span>                Console.WriteLine(<span style="color:#e6db74">$&#34;Parsed Correctly&#34;</span>);
</span></span><span style="display:flex;"><span>            }
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">catch</span>
</span></span><span style="display:flex;"><span>            {
</span></span><span style="display:flex;"><span>                Console.WriteLine(<span style="color:#e6db74">$&#34;Failed to parse Json&#34;</span>);
</span></span><span style="display:flex;"><span>            }
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        FPDFTextClosePage(pageTextT); <span style="color:#75715e">//liberar pagina</span>
</span></span><span style="display:flex;"><span>        FPDF_ClosePage(pageT); <span style="color:#75715e">//liberar texto</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    FPDF_CloseDocument(documenT); <span style="color:#75715e">//liberar documento</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>FPDF_DestroyLibrary();
</span></span></code></pre></div><h2 id="resultados">Resultados</h2>
<p>En mis pruebas, he encontrado que algunos modelos simplemente no funcionan como se esperaba. Por ejemplo, al utilizar este modelo, la respuesta fue completamente inadecuada.</p>
<p>Llama 3.2 me respondía con estructuras de tool, pero no respetaban la estructura que esperaba. Quería ver si podíamos utilizar esto para que, por ejemplo, un usuario definiera una extracción con los datos que él espera, y nosotros simplemente llamarlo y esperar que nos regresara los datos correctos. En este caso, ya nos dimos cuenta de que, por ejemplo, OpenAI no se ha equivocado ninguna vez, a pesar de que es un modelo pequeño y muy económico.</p>
<p>En otros casos cuando el modelo agarra el tool, no me responde con los conceptos que realmente necesito. En su lugar, recibo un esquema completo que incluye parámetros, conceptos y descripciones, pero pueden ser cadenas vacias o nulas.</p>
<p>Al final, lo que podría sugerir en última instancia es que le indique que extraiga los conceptos utilizando el formato JSON. Le he estado dando ejemplos, y es probable que eso lo ayude. Estoy tratando de asegurarme de que el esquema que envíe sea el correcto.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span>ChatMessage[] chatMessages = [
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">new</span> SystemChatMessage(<span style="color:#e6db74">&#34;Eres un bot de extraccion de datos de catalogos de conceptos, responde solo con json valido.&#34;</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">new</span> UserChatMessage(<span style="color:#e6db74">$@&#34;Con los siguientes datos de un catalogo de conceptos \n```\n{pageText}\n```\n 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Extrae una lista de conceptos con un arreglo de clave y descripcion con el formato de json: {{conceptos: [{{clave: &#34;&#34;Clave del concepto&#34;&#34;, descripcion: &#34;&#34;Descripcion del concepto&#34;&#34;}}]}}.&#34;</span>)
</span></span><span style="display:flex;"><span>];
</span></span></code></pre></div><p>He estado probando diferentes formas de estructurar el JSON, tratando de definir un arreglo de conceptos y varias maneras de facilitarle el trabajo al modelo, pero los resultados no son favorables.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Hasta ahora, el modelo más pequeño que ha funcionado en casi todos nuestros casos es el de <strong>GPT-4o-mini</strong>. Esta investigación es crucial para encontrar modelos que ofrezcan mejores resultados a precios más bajos, y en este caso, este modelo se planteaba como una buena opción para poder realizar aplicaciones con IA generativa en hardware de consumo general. Es sorprendente lo que puede hacer este modelo, pero por ahora no es un modelo adecuado para nuestras necesidades de extracción de datos.</p>

</content>
<p>
  
  <a href="/tags/ai/">#Ai</a>
  
  <a href="/tags/csharp/">#Csharp</a>
  
  <a href="/tags/llm/">#Llm</a>
  
  <a href="/tags/openai/">#Openai</a>
  
  <a href="/tags/programacion/">#Programacion</a>
  
  <a href="/tags/llama/">#Llama</a>
  
  <a href="/tags/llama3.2/">#Llama3.2</a>
  
</p>

  </main>
  <footer><a href="https://github.com/janraasch/hugo-bearblog/">ʕ•ᴥ•ʔ</a>
</footer>

    
</body>

</html>
