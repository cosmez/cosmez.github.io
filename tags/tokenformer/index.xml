<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tokenformer on λ Cosme Zamudio</title>
    <link>/tags/tokenformer/</link>
    <description>Recent content in Tokenformer on λ Cosme Zamudio</description>
    <generator>Hugo</generator>
    <language>es-MX</language>
    <lastBuildDate>Sat, 09 Nov 2024 16:17:11 -0700</lastBuildDate>
    <atom:link href="/tags/tokenformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Tokenformer</title>
      <link>/blog/tokenformer/</link>
      <pubDate>Sat, 09 Nov 2024 16:17:11 -0700</pubDate>
      <guid>/blog/tokenformer/</guid>
      <description>&lt;h2 id=&#34;tokenformer&#34;&gt;TokenFormer&lt;/h2&gt;&#xA;&lt;p&gt;La arquitectura Transformer han dominado la inteligencia artificial gracias a su capacidad para manejar datos y relaciones complejas. Sin embargo, enfrentan dos grandes desafíos: la capacidad de aprender en tiempo real y el alto costo energético de su entrenamiento. &lt;a href=&#34;https://arxiv.org/abs/2410.23168&#34;&gt;TokenFormer&lt;/a&gt; (wang2024tokenformer) es una nueva arquitectura diseñada para resolver estos problemas.&lt;/p&gt;&#xA;&lt;h3 id=&#34;neuroplasticidad-en-redes-neuronales&#34;&gt;Neuroplasticidad en Redes Neuronales&lt;/h3&gt;&#xA;&lt;p&gt;Uno de los mayores desafíos en la evolución de las redes neuronales es su limitada neuroplasticidad, es decir, la capacidad de integrar conocimiento nuevo sin perder lo ya aprendido. TokenFormer soluciona esto reutilizando parámetros previamente entrenados y añadiendo nuevos, lo que permite un aprendizaje progresivo sin necesidad de reentrenar desde cero.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
